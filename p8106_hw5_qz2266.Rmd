---
title: "p8106_hw5_qz2266"
author: "Qing Zhou"
date: "2023-05-01"
output: pdf_document
---

```{r, include = FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(caret)
library(e1071)
library(ISLR) 
library(factoextra)

knitr::opts_chunk$set(warning = FALSE)
```

## Question 1

In this problem, we will apply support vector machines to predict whether a given car gets high or low gas mileage based on the dataset “auto.csv”. The response variable is mpg cat. The predictors are cylinders, displacement, horsepower, weight, acceleration, year, and origin.

```{r setup}
# Data import and preparation
auto = read.csv("data/auto.csv") %>% 
  mutate(
    mpg_cat = as.factor(mpg_cat),
    mpg_cat = fct_relevel(mpg_cat, c("low", "high")),
    year = factor(year),
    origin = as.factor(origin)) 
```

Split the dataset into two parts - training data (70%) and test data (30%).

```{r}
set.seed(1)
# Data partition
trainRows <- createDataPartition(y = auto$mpg_cat, p = 0.7,list = FALSE)
auto_train = auto[trainRows, ]
auto_test = auto[-trainRows, ]
```

Here I mutated `year` as a factor variable, since I don't assume there's a linear relationship between model year and gas millage. 


### (a) Fit a support vector classifier (linear kernel) to the training data. 

```{r}
set.seed(1)
# Fit model
linear.tune <- tune.svm(mpg_cat ~ . ,
                        data = auto_train,
                        kernel = "linear",
                        cost = exp(seq(-5,2,len = 50)), 
                        scale = TRUE) 
plot(linear.tune)

# Optimal tuning parameters
linear.tune$best.parameters

# Extract final model and summarize
best.linear <- linear.tune$best.model
summary(best.linear)

# Report training error rate
confusionMatrix(data = best.linear$fitted,
                reference = auto_train$mpg_cat)

# Report test error rate 
pred.linear <- predict(best.linear, newdata = auto_test)
confusionMatrix(data = pred.linear,
                reference = auto_test$mpg_cat)
```

- With cross-validation, we find the optimal tuning parameter is when cost is 1.535063, which minimizes the error. There are 62 support vectors in the optimal support vector classifier with a linear kernel.

- According to the confusion Matrix above, for the training data, the accuracy of the fitted support vector classifier is 0.9384, so the training error rate is (1-0.9384)x100% = 6.16% .

- According to the confusion Matrix above, the accuracy when applied the model to the test data is 0.9052, so the test error rate is (1-0.9052)x100% = 9.48% .



### b) Fit a support vector machine with a radial kernel to the training data.

```{r}
set.seed(1)
# Fit model
radial.tune <- tune.svm(mpg_cat ~ .,
                        data = auto_train,
                        kernel = "radial",
                        cost = exp(seq(-1,4,len = 20)),
                        gamma = exp(seq(-6,-2,len = 20)))
plot(radial.tune, transform.y = log, transform.x = log,
     color.palette = terrain.colors)

# Optimal tuning parameters
radial.tune$best.parameters

# Extract final model and summarize
best.radial <- radial.tune$best.model
summary(best.radial)

# Report training error rate
confusionMatrix(data = best.radial$fitted,
                reference = auto_train$mpg_cat)

# Report test error rate
pred.radial <- predict(best.radial, newdata = auto_test)
confusionMatrix(data = pred.radial,
                reference = auto_test$mpg_cat)
```

- From the summary, we found the optimal tuning parameters, gamma and cost, of the support vector machine are 0.1353353 and 6.650798. There are 72 support vectors in the optimal support vector classifier with a linear kernel. 

- We use the best model to determine our training and testing error rates. The training error rate for the support vector machine is (1-0.9783)x100% = 2.17%. The test error rate is (1-0.9138)x100% = 8.62%.



## Question 2

We perform hierarchical clustering on the states using the USArrests data in the ISLR package. For each of the 50 states in the United States, the dataset contains the number of arrests per 100,000 residents for each of three crimes: Assault, Murder, and Rape. The dataset also contains the percent of the population in each state living in urban areas, UrbanPop. The four variables will be used as features for clustering.

```{r, results='hide'}
# import data
data(USArrests)
arrests_df = USArrests %>%
  as.data.frame() %>%
  janitor::clean_names()
```

### a) Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states. Cut the dendrogram at a height that results in three distinct clusters.

```{r}
# compute the 3 clusters of states
hc.complete <- hclust(dist(arrests_df), method = "complete")

# visualize
fviz_dend(hc.complete, k = 3,
          cex = 0.3,
          palette = "jco",
          color_labels_by_k = TRUE,
          rect = TRUE, rect_fill = TRUE, rect_border = "jco",
          labels_track_height = 2.5)

ind3.complete <- cutree(hc.complete, 3)

# The states in different clusters
clus1 <- rownames(arrests_df[ind3.complete == 1,]); clus1
clus2 <- rownames(arrests_df[ind3.complete == 2,]); clus2
clus3 <- rownames(arrests_df[ind3.complete == 3,]); clus3

```

- The states in cluster 1 include Alabama, Alaska, Arizona, California, Delaware, Florida, Illinois, Louisiana, Maryland, Michigan, Mississippi, Nevada, New Mexico, New York, North Carolina, and South Carolina.

- The states in cluster 2 include Arkansas, Colorado, Georgia, Massachusetts, Missouri, New Jersey, Oklahoma, Oregon, Rhode Island, Tennessee, Texas, Virginia, Washington, and Wyoming.

- The states in cluster 3 include Connecticut, Hawaii, Idaho, Indiana, Iowa, Kansas, Kentucky, Maine, Minnesota, Montana, Nebraska, New Hampshire, North Dakota, Ohio, Pennsylvania, South Dakota, Utah, Vermont, West Virginia and Wisconsin.


### b) Hierarchically cluster the states using complete linkage and Euclidean distance, after scaling the variables to have standard deviation one.

```{r}
arrests_df_scaled = scale(arrests_df)

hc.complete.scaled <- hclust(dist(arrests_df_scaled), method = "complete")
fviz_dend(hc.complete.scaled, k = 3,
          cex = 0.3,
          palette = "jco",
          color_labels_by_k = TRUE,
          rect = TRUE, rect_fill = TRUE, rect_border = "jco",
          labels_track_height = 2.5)

ind3.complete.scaled <- cutree(hc.complete.scaled, 3)

# The states in different clusters for standardized data
scaled.clus1 <- rownames(arrests_df[ind3.complete.scaled == 1,]); scaled.clus1
scaled.clus2 <- rownames(arrests_df[ind3.complete.scaled == 2,]); scaled.clus2
scaled.clus3 <- rownames(arrests_df[ind3.complete.scaled == 3,]); scaled.clus3
```

- The states in cluster 1 include Alabama, Alaska, Georgia, Louisiana, Mississippi, North Carolina, South Carolina, and Tennessee.

- The states in cluster 2 include Arizona, California, Colorado, Florida, Illinois, Maryland, Michigan, Nevada, New Mexico, New York, and Texas.

- The states in cluster 3 include Arkansas, Connecticut, Delaware, Hawaii, Idaho, Indiana, Iowa, Kansas, Kentucky, Maine, Massachusetts, Minnesota, Missouri, Montana, Nebraska, New Hampshire, New Jersey, North Dakota, Ohio, Oklahoma, Oregon, Pennsylvania, Rhode Island, South Dakota, Utah, Vermont, Virginia, Washington, West Virginia, Wisconsin, and Wyoming.


### c). Does scaling the variables change the clustering results? Why? In your opinion, should the variables be scaled before the inter-observation dissimilarities are computed?

```{r}
skimr::skim_without_charts(arrests_df)
```

- Yes, scaling the variables does change the clustering results. In the scaled dataset, cluster 3 encompasses a greater number of states than cluster 3 in the non-scaled dataset. Additionally, the remaining two clusters also consist of different states after scaling the variables.


- The reason is,  when the variables in a dataset have different scales, the distance metric used in clustering can be dominated by the variables with the largest scales. To avoid bias towards these variables, it can be beneficial to scale all variables to the same scale. This will allow each variable to contribute equally to the distance metric and help prevent the clustering algorithm from being biased towards variables with larger scales. This problem employs the Euclidean distance as the distance metric, but the variable urban_pop (percent of the population in each state living in urban areas) in the dataset has units that are incomparable to the other variables such as murder, and rape (number of arrests per 100,000 residents for each of the three crimes).


- In general, whether scaling the variables or not depends on the specific problem and the nature of the data. As for this problem, in my opinion, the variables should be scaled before the inter-observation dissimilarities are computed. By doing so, we can avoid unfairly assigning more significance to the variables with larger magnitudes or incomparable units, such as urban_pop.
 




